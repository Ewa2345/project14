# -*- coding: utf-8 -*-
"""nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lWpuuS7F01kYsSsekHLIvaUxbELq6N2w
"""

import numpy as np 
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt
import re
import nltk
import io
from sklearn import metrics
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
import sklearn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.model_selection import cross_val_score

from google.colab import files
uploaded = files.upload()

data=uploaded['Corona_NLP_train.csv']

df=pd.read_csv(io.StringIO(data.decode('latin1')))

df.head()

df.info()

reg = re.compile("(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([^0-9A-Za-z t])|(w+://S+)")
tweet = []
for i in df["OriginalTweet"]:
    tweet.append(reg.sub(" ", i))

tweet

df = pd.concat([df, pd.DataFrame(tweet, columns=["CleanedTweet"])], axis=1, sort=False)

df.head(10)

tokenizer=RegexpTokenizer(r'\w+')

df['CleanedTweet']=df['CleanedTweet'].apply(lambda x: tokenizer.tokenize(x.lower()))

df.head()

def remove_stopwords(text):
    words = [w for w in text if w not in stopwords.words('english')]
    return words

df['CleanedTweet']=df['CleanedTweet'].apply(lambda x: remove_stopwords(x))

df.head()

from nltk.stem import LancasterStemmer
stemmer=LancasterStemmer()

def word_stemmer(text):
    stem_text=" ".join([stemmer.stem(i) for i in text])
    return stem_text

df['CleanedTweet']=df['CleanedTweet'].apply(lambda x: word_stemmer(x))

df.head()

lemmatizer=WordNetLemmatizer()

def word_lemmatizer(text):
    lem_text = [lemmatizer.lemmatize(i) for i in text]
    return lem_text

df['CleanedTweet']=df['CleanedTweet'].apply(lambda x: word_lemmatizer(x))

from sklearn.feature_extraction.text import TfidfVectorizer
stop_words = set(stopwords.words('english'))     
vectoriser = TfidfVectorizer(stop_words=None)

X_train = vectoriser.fit_transform(df["CleanedTweet"])
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y_train = encoder.fit_transform(df['Sentiment'])
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(X_train, y_train)

test=uploaded['Corona_NLP_test.csv']
test_df = pd.read_csv(io.StringIO(test.decode('latin1')))

test_df.head()

reg1 = re.compile("(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([^0-9A-Za-z t])|(w+://S+)")
tweet = []
for i in test_df["OriginalTweet"]:
    tweet.append(reg1.sub(" ", i))
test_df = pd.concat([test_df, pd.DataFrame(tweet, columns=["CleanedTweet"])], axis=1, sort=False)
test_df.head()

X_test = vectoriser.transform(test_df["CleanedTweet"])
y_test = encoder.transform(test_df["Sentiment"])
y_pred = classifier.predict(X_test)
pred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
pred_df.head()

from sklearn import metrics
# Generate the roc curve using scikit-learn.
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)
plt.plot(fpr, tpr)
plt.xlabel('Nieprawidłowe dopasownie')
plt.ylabel('Prawidłowe dopasownie')
plt.title('Krzywa ROC')
plt.show()
print("Dokladnosc predykcji: {0}".format(metrics.auc(fpr, tpr)))

"""Im bliżej 1, tym „lepsza” predykcja. Otrzymałam dopasowanie około 60%, czyli słabo"""



